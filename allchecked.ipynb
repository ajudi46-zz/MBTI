{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "import random\n",
    "import collections\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "df = pd.read_csv('mbti_1.csv')\n",
    "\n",
    "types = ('ISTJ', 'ISFJ', 'INFJ', 'INTJ', \\\n",
    "\t\t 'ISTP', 'ISFP', 'INFP', 'INTP', \\\n",
    "\t\t 'ESTP', 'ESFP', 'ENFP', 'ENTP', \\\n",
    "\t\t 'ESTJ', 'ESFJ', 'ENFJ', 'ENTJ')\n",
    "\n",
    "gen_pop = {'ISTJ': 0.11, 'ISFJ': 0.09, 'INFJ': 0.04, 'INTJ': 0.05, \\\n",
    "\t\t   'ISTP': 0.05, 'ISFP': 0.05, 'INFP': 0.06, 'INTP': 0.06, \\\n",
    "\t\t   'ESTP': 0.04, 'ESFP': 0.04, 'ENFP': 0.08, 'ENTP': 0.06, \\\n",
    "\t\t   'ESTJ': 0.08, 'ESFJ': 0.09, 'ENFJ': 0.05, 'ENTJ': 0.05}\n",
    "\n",
    "n, ___ = df.shape\n",
    "\n",
    "counts = collections.defaultdict(int)\n",
    "for mbti in df['type']:\n",
    "\tcounts[mbti] += 1\n",
    "\n",
    "limiting_type = None\n",
    "min_size = float('infinity')\n",
    "for mbti in counts.keys():\n",
    "\tsize = counts[mbti]/gen_pop[mbti]\n",
    "\tif size < min_size:\n",
    "\t\tmin_size = size\n",
    "\t\tlimiting_type = mbti\n",
    "\n",
    "dic = collections.defaultdict(list)\n",
    "for row in df.iterrows():\n",
    "\tdic[row[1]['type']].append(row)\n",
    "\n",
    "unclean_list = []\n",
    "\n",
    "\n",
    "with open('mbti_clean.csv', 'w', encoding = 'UTF-8') as f:\n",
    "\twriter = csv.writer(f)\n",
    "\twriter.writerow(['type', 'posts'])\n",
    "\t\n",
    "\tfor mbti in gen_pop.keys():\n",
    "\t\tlist1 = dic[mbti]\n",
    "\t\tfor x in range(0, int(round(min_size*gen_pop[mbti]))):\n",
    "\t\t\twriter.writerow(list1[x][1])\t\n",
    "\t\tunclean_list.append(list1[int(round(min_size*gen_pop[mbti])) : len(list1)])\n",
    "\t\t\t\t\t\n",
    "with open('mbti_unclean.csv', 'w', encoding = 'UTF-8') as f:\n",
    "\twriter = csv.writer(f)\n",
    "\twriter.writerow(['type', 'posts'])\n",
    "\tfor mbti in unclean_list:\n",
    "\t\tfor x in mbti:\n",
    "\t\t\twriter.writerow(x[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import random\n",
    "import collections\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import re\n",
    "\n",
    "df = pd.read_csv('mbti_unclean.csv')\n",
    "DIMENSIONS = ('IE', 'NS', 'TF', 'PJ')\n",
    "counts = collections.defaultdict(int)\n",
    "\n",
    "for dimension in DIMENSIONS: \n",
    "\tletter_1, letter_2 = dimension\n",
    "\tfor mbti in df['type']:\n",
    "\t\tif letter_1 in mbti:\n",
    "\t\t\tcounts[letter_1] += 1\n",
    "\t\tif letter_2 in mbti:\n",
    "\t\t\tcounts[letter_2] += 1\n",
    "\n",
    "for dimension in DIMENSIONS: \n",
    "\tletter_1, letter_2 = dimension\n",
    "\tif counts[letter_1] < counts[letter_2]:\n",
    "\t\tlimit = counts[letter_1]\n",
    "\telse: \n",
    "\t\tlimit = counts[letter_2]\n",
    "\tlist1 = []\n",
    "\tlist2 = []\n",
    "\ti = 0\n",
    "\tfor row in df.iterrows():\t\n",
    "\t\tif i == limit: break\n",
    "\t\tif letter_1 in row[1]['type']:\n",
    "\t\t\tlist1.append(row[1]['posts'].split('|||'))\n",
    "\t\t\ti += 1\n",
    "\ti = 0\n",
    "\tfor row in df.iterrows():\t\n",
    "\t\tif i == limit: break\n",
    "\t\tif letter_2 in row[1]['type']:\n",
    "\t\t\tlist2.append(row[1]['posts'].split('|||'))\t\n",
    "\t\t\ti += 1\n",
    "\twith open('train_' + letter_1 + '.csv', 'w',encoding = 'UTF-8') as f:\n",
    "\t\twriter = csv.writer(f)\n",
    "\t\tfor hundred_posts in list1:\n",
    "\t\t\trow = [post for post in hundred_posts if ('http' not in post) and (post != '') and (post != None) and (re.search(\"[a-zA-Z]\", post))]\n",
    "\t\t\twriter.writerow(row)\n",
    "\twith open('train_' + letter_2 + '.csv', 'w',encoding = 'UTF-8') as f:\n",
    "\t\twriter = csv.writer(f)\n",
    "\t\tfor hundred_posts in list2:\n",
    "\t\t\trow = [post for post in hundred_posts if ('http' not in post) and (post != '') and (post != None) and (re.search(\"[a-zA-Z]\", post))]\n",
    "\t\t\twriter.writerow(row)\n",
    "\n",
    "\n",
    "\n",
    "\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "import random\n",
    "import collections\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import re\n",
    "\n",
    "letters = ('I', 'E', 'N', 'S', 'T', 'F', 'P', 'J')\n",
    "df = pd.read_csv('mbti_clean.csv')\n",
    "test = collections.defaultdict(list) \n",
    "\n",
    "for row in df.iterrows():\n",
    "\tposts = row[1]['posts'].split('|||')\n",
    "\ttest[row[1]['type']].append(posts)\n",
    "\n",
    "### write csv files for every every letter class and train vs. test class (16 total)\n",
    "for letter in letters: \n",
    "\tPATH = 'test_' + letter + '.csv'\n",
    "\twith open(PATH, 'w',encoding = 'UTF-8') as f:\n",
    "\t\twriter = csv.writer(f)\n",
    "\t\tfor key in test.keys():\n",
    "\t\t\tif letter in key:\n",
    "\t\t\t\tfor hundred_posts in test[key]:\n",
    "\t\t\t\t\trow = [post for post in hundred_posts if ('http' not in post) and (post != '') and (post != None) and (re.search(\"[a-zA-Z]\", post))]\n",
    "\t\t\t\t\tif len(row) > 10: \n",
    "\t\t\t\t\t\twriter.writerow(row)\n",
    "\n",
    "\n",
    "\n",
    "\t\t\t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ajudi46/anaconda3/envs/pronew/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/ajudi46/anaconda3/envs/pronew/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/ajudi46/anaconda3/envs/pronew/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/ajudi46/anaconda3/envs/pronew/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/ajudi46/anaconda3/envs/pronew/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/ajudi46/anaconda3/envs/pronew/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/ajudi46/anaconda3/envs/pronew/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/ajudi46/anaconda3/envs/pronew/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/ajudi46/anaconda3/envs/pronew/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/ajudi46/anaconda3/envs/pronew/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/ajudi46/anaconda3/envs/pronew/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/ajudi46/anaconda3/envs/pronew/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/ajudi46/anaconda3/envs/pronew/lib/python3.7/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400000 word vectors.\n",
      "WARNING:tensorflow:From /home/ajudi46/anaconda3/envs/pronew/lib/python3.7/site-packages/tensorflow/python/keras/backend.py:3794: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 40, 50)            125000    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 50)                20200     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 145,251\n",
      "Trainable params: 145,251\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "WARNING:tensorflow:From /home/ajudi46/anaconda3/envs/pronew/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Epoch 1/30\n",
      "162776/162776 [==============================] - 87s 536us/step - loss: 0.6890 - accuracy: 0.5351\n",
      "Epoch 2/30\n",
      "162776/162776 [==============================] - 73s 447us/step - loss: 0.6779 - accuracy: 0.5693\n",
      "Epoch 3/30\n",
      "162776/162776 [==============================] - 73s 447us/step - loss: 0.6726 - accuracy: 0.5804\n",
      "Epoch 4/30\n",
      "162776/162776 [==============================] - 75s 459us/step - loss: 0.6684 - accuracy: 0.5880\n",
      "Epoch 5/30\n",
      "162776/162776 [==============================] - 75s 460us/step - loss: 0.6643 - accuracy: 0.5948\n",
      "Epoch 6/30\n",
      "162776/162776 [==============================] - 75s 461us/step - loss: 0.6607 - accuracy: 0.6002\n",
      "Epoch 7/30\n",
      "162776/162776 [==============================] - 75s 460us/step - loss: 0.6576 - accuracy: 0.6039\n",
      "Epoch 8/30\n",
      "162776/162776 [==============================] - 75s 461us/step - loss: 0.6550 - accuracy: 0.6071\n",
      "Epoch 9/30\n",
      "162776/162776 [==============================] - 75s 461us/step - loss: 0.6522 - accuracy: 0.6115\n",
      "Epoch 10/30\n",
      "162776/162776 [==============================] - 77s 473us/step - loss: 0.6502 - accuracy: 0.6130\n",
      "Epoch 11/30\n",
      "162776/162776 [==============================] - 77s 473us/step - loss: 0.6479 - accuracy: 0.6151\n",
      "Epoch 12/30\n",
      "162776/162776 [==============================] - 77s 473us/step - loss: 0.6463 - accuracy: 0.6180\n",
      "Epoch 13/30\n",
      "162776/162776 [==============================] - 77s 474us/step - loss: 0.6442 - accuracy: 0.6199\n",
      "Epoch 14/30\n",
      "162776/162776 [==============================] - 77s 475us/step - loss: 0.6428 - accuracy: 0.6217\n",
      "Epoch 15/30\n",
      "162776/162776 [==============================] - 77s 473us/step - loss: 0.6414 - accuracy: 0.6239\n",
      "Epoch 16/30\n",
      "162776/162776 [==============================] - 77s 474us/step - loss: 0.6393 - accuracy: 0.6271\n",
      "Epoch 17/30\n",
      "162776/162776 [==============================] - 77s 475us/step - loss: 0.6385 - accuracy: 0.6284\n",
      "Epoch 18/30\n",
      "162776/162776 [==============================] - 77s 474us/step - loss: 0.6377 - accuracy: 0.6282\n",
      "Epoch 19/30\n",
      "162776/162776 [==============================] - 77s 475us/step - loss: 0.6367 - accuracy: 0.6293\n",
      "Epoch 20/30\n",
      "162776/162776 [==============================] - 77s 474us/step - loss: 0.6366 - accuracy: 0.6290\n",
      "Epoch 21/30\n",
      "162776/162776 [==============================] - 77s 475us/step - loss: 0.6345 - accuracy: 0.6319\n",
      "Epoch 22/30\n",
      "162776/162776 [==============================] - 77s 475us/step - loss: 0.6336 - accuracy: 0.6325\n",
      "Epoch 23/30\n",
      "162776/162776 [==============================] - 77s 474us/step - loss: 0.6333 - accuracy: 0.6326\n",
      "Epoch 24/30\n",
      "162776/162776 [==============================] - 77s 476us/step - loss: 0.6327 - accuracy: 0.6332\n",
      "Epoch 25/30\n",
      "162776/162776 [==============================] - 79s 486us/step - loss: 0.6326 - accuracy: 0.6328\n",
      "Epoch 26/30\n",
      "162776/162776 [==============================] - 77s 475us/step - loss: 0.6319 - accuracy: 0.6341\n",
      "Epoch 27/30\n",
      "162776/162776 [==============================] - 77s 475us/step - loss: 0.6310 - accuracy: 0.6351\n",
      "Epoch 28/30\n",
      "162776/162776 [==============================] - 77s 476us/step - loss: 0.6312 - accuracy: 0.6351\n",
      "Epoch 29/30\n",
      "162776/162776 [==============================] - 78s 480us/step - loss: 0.6308 - accuracy: 0.6341\n",
      "Epoch 30/30\n",
      "162776/162776 [==============================] - 77s 476us/step - loss: 0.6302 - accuracy: 0.6356\n",
      "Loaded 400000 word vectors.\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 40, 50)            125000    \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 50)                20200     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 145,251\n",
      "Trainable params: 145,251\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/30\n",
      "85219/85219 [==============================] - 41s 479us/step - loss: 0.6858 - accuracy: 0.5476\n",
      "Epoch 2/30\n",
      "85219/85219 [==============================] - 40s 473us/step - loss: 0.6676 - accuracy: 0.5935\n",
      "Epoch 3/30\n",
      "85219/85219 [==============================] - 41s 475us/step - loss: 0.6570 - accuracy: 0.6081\n",
      "Epoch 4/30\n",
      "85219/85219 [==============================] - 41s 475us/step - loss: 0.6487 - accuracy: 0.6190\n",
      "Epoch 5/30\n",
      "85219/85219 [==============================] - 40s 474us/step - loss: 0.6395 - accuracy: 0.6293\n",
      "Epoch 6/30\n",
      "85219/85219 [==============================] - 41s 475us/step - loss: 0.6311 - accuracy: 0.6362\n",
      "Epoch 7/30\n",
      "85219/85219 [==============================] - 40s 475us/step - loss: 0.6229 - accuracy: 0.6455\n",
      "Epoch 8/30\n",
      "85219/85219 [==============================] - 40s 474us/step - loss: 0.6156 - accuracy: 0.6502\n",
      "Epoch 9/30\n",
      "85219/85219 [==============================] - 41s 477us/step - loss: 0.6080 - accuracy: 0.6574\n",
      "Epoch 10/30\n",
      "85219/85219 [==============================] - 40s 474us/step - loss: 0.6016 - accuracy: 0.6616\n",
      "Epoch 11/30\n",
      "85219/85219 [==============================] - 41s 476us/step - loss: 0.5959 - accuracy: 0.6676\n",
      "Epoch 12/30\n",
      "85219/85219 [==============================] - 41s 476us/step - loss: 0.5897 - accuracy: 0.6733\n",
      "Epoch 13/30\n",
      "85219/85219 [==============================] - 40s 474us/step - loss: 0.5841 - accuracy: 0.6778\n",
      "Epoch 14/30\n",
      "85219/85219 [==============================] - 40s 475us/step - loss: 0.5798 - accuracy: 0.6794\n",
      "Epoch 15/30\n",
      "85219/85219 [==============================] - 41s 476us/step - loss: 0.5743 - accuracy: 0.6861\n",
      "Epoch 16/30\n",
      "85219/85219 [==============================] - 40s 475us/step - loss: 0.5713 - accuracy: 0.6894\n",
      "Epoch 17/30\n",
      "85219/85219 [==============================] - 40s 475us/step - loss: 0.5681 - accuracy: 0.6908\n",
      "Epoch 18/30\n",
      "85219/85219 [==============================] - 41s 478us/step - loss: 0.5634 - accuracy: 0.6952\n",
      "Epoch 19/30\n",
      "85219/85219 [==============================] - 41s 475us/step - loss: 0.5610 - accuracy: 0.6980\n",
      "Epoch 20/30\n",
      "85219/85219 [==============================] - 40s 475us/step - loss: 0.5576 - accuracy: 0.6996\n",
      "Epoch 21/30\n",
      "85219/85219 [==============================] - 41s 476us/step - loss: 0.5532 - accuracy: 0.7043\n",
      "Epoch 22/30\n",
      "85219/85219 [==============================] - 41s 476us/step - loss: 0.5504 - accuracy: 0.7067\n",
      "Epoch 23/30\n",
      "85219/85219 [==============================] - 40s 475us/step - loss: 0.5492 - accuracy: 0.7071\n",
      "Epoch 24/30\n",
      "85219/85219 [==============================] - 41s 476us/step - loss: 0.5471 - accuracy: 0.7085\n",
      "Epoch 25/30\n",
      "85219/85219 [==============================] - 40s 474us/step - loss: 0.5452 - accuracy: 0.7117\n",
      "Epoch 26/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85219/85219 [==============================] - 36s 421us/step - loss: 0.5415 - accuracy: 0.7143\n",
      "Epoch 27/30\n",
      "85219/85219 [==============================] - 36s 422us/step - loss: 0.5410 - accuracy: 0.7146\n",
      "Epoch 28/30\n",
      "85219/85219 [==============================] - 37s 429us/step - loss: 0.5397 - accuracy: 0.7141\n",
      "Epoch 29/30\n",
      "85219/85219 [==============================] - 36s 421us/step - loss: 0.5379 - accuracy: 0.7164\n",
      "Epoch 30/30\n",
      "85219/85219 [==============================] - 36s 420us/step - loss: 0.5355 - accuracy: 0.7163\n",
      "Loaded 400000 word vectors.\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 40, 50)            125000    \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 50)                20200     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 145,251\n",
      "Trainable params: 145,251\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/30\n",
      "340841/340841 [==============================] - 153s 448us/step - loss: 0.6724 - accuracy: 0.5805\n",
      "Epoch 2/30\n",
      "340841/340841 [==============================] - 154s 451us/step - loss: 0.6635 - accuracy: 0.5972\n",
      "Epoch 3/30\n",
      "340841/340841 [==============================] - 154s 451us/step - loss: 0.6607 - accuracy: 0.6020\n",
      "Epoch 4/30\n",
      "340841/340841 [==============================] - 155s 454us/step - loss: 0.6585 - accuracy: 0.6055\n",
      "Epoch 5/30\n",
      "340841/340841 [==============================] - 155s 454us/step - loss: 0.6571 - accuracy: 0.6072\n",
      "Epoch 6/30\n",
      "340841/340841 [==============================] - 155s 456us/step - loss: 0.6559 - accuracy: 0.6086\n",
      "Epoch 7/30\n",
      "340841/340841 [==============================] - 158s 464us/step - loss: 0.6554 - accuracy: 0.6099\n",
      "Epoch 8/30\n",
      "340841/340841 [==============================] - 159s 465us/step - loss: 0.6549 - accuracy: 0.6101\n",
      "Epoch 9/30\n",
      "340841/340841 [==============================] - 158s 465us/step - loss: 0.6542 - accuracy: 0.6108\n",
      "Epoch 10/30\n",
      "340841/340841 [==============================] - 159s 466us/step - loss: 0.6540 - accuracy: 0.6115\n",
      "Epoch 11/30\n",
      "340841/340841 [==============================] - 159s 466us/step - loss: 0.6532 - accuracy: 0.6124\n",
      "Epoch 12/30\n",
      "340841/340841 [==============================] - 157s 462us/step - loss: 0.6530 - accuracy: 0.6129\n",
      "Epoch 13/30\n",
      "340841/340841 [==============================] - 162s 475us/step - loss: 0.6528 - accuracy: 0.6133\n",
      "Epoch 14/30\n",
      "340841/340841 [==============================] - 156s 458us/step - loss: 0.6524 - accuracy: 0.6139\n",
      "Epoch 15/30\n",
      "340841/340841 [==============================] - 155s 456us/step - loss: 0.6522 - accuracy: 0.6127\n",
      "Epoch 16/30\n",
      "340841/340841 [==============================] - 156s 457us/step - loss: 0.6524 - accuracy: 0.6139\n",
      "Epoch 17/30\n",
      "340841/340841 [==============================] - 155s 456us/step - loss: 0.6516 - accuracy: 0.6144\n",
      "Epoch 18/30\n",
      "340841/340841 [==============================] - 155s 456us/step - loss: 0.6516 - accuracy: 0.6139\n",
      "Epoch 19/30\n",
      "340841/340841 [==============================] - 159s 467us/step - loss: 0.6511 - accuracy: 0.6154\n",
      "Epoch 20/30\n",
      "340841/340841 [==============================] - 145s 425us/step - loss: 0.6511 - accuracy: 0.6156\n",
      "Epoch 21/30\n",
      "340841/340841 [==============================] - 144s 424us/step - loss: 0.6511 - accuracy: 0.6159\n",
      "Epoch 22/30\n",
      "340841/340841 [==============================] - 149s 436us/step - loss: 0.6509 - accuracy: 0.6155\n",
      "Epoch 23/30\n",
      "340841/340841 [==============================] - 156s 456us/step - loss: 0.6515 - accuracy: 0.6152\n",
      "Epoch 24/30\n",
      "340841/340841 [==============================] - 156s 458us/step - loss: 0.6513 - accuracy: 0.6149\n",
      "Epoch 25/30\n",
      "340841/340841 [==============================] - 159s 466us/step - loss: 0.6512 - accuracy: 0.6155\n",
      "Epoch 26/30\n",
      "340841/340841 [==============================] - 159s 467us/step - loss: 0.6508 - accuracy: 0.6149\n",
      "Epoch 27/30\n",
      "340841/340841 [==============================] - 160s 468us/step - loss: 0.6508 - accuracy: 0.6155\n",
      "Epoch 28/30\n",
      "340841/340841 [==============================] - 159s 467us/step - loss: 0.6506 - accuracy: 0.6160\n",
      "Epoch 29/30\n",
      "340841/340841 [==============================] - 158s 464us/step - loss: 0.6509 - accuracy: 0.6156\n",
      "Epoch 30/30\n",
      "340841/340841 [==============================] - 156s 457us/step - loss: 0.6507 - accuracy: 0.6159\n",
      "Loaded 400000 word vectors.\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 40, 50)            125000    \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 50)                20200     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 145,251\n",
      "Trainable params: 145,251\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/30\n",
      "288107/288107 [==============================] - 138s 479us/step - loss: 0.6910 - accuracy: 0.5289\n",
      "Epoch 2/30\n",
      "288107/288107 [==============================] - 137s 476us/step - loss: 0.6862 - accuracy: 0.5482\n",
      "Epoch 3/30\n",
      "288107/288107 [==============================] - 138s 477us/step - loss: 0.6839 - accuracy: 0.5557\n",
      "Epoch 4/30\n",
      "288107/288107 [==============================] - 139s 484us/step - loss: 0.6821 - accuracy: 0.5603\n",
      "Epoch 5/30\n",
      "288107/288107 [==============================] - 138s 478us/step - loss: 0.6805 - accuracy: 0.5638\n",
      "Epoch 6/30\n",
      "288107/288107 [==============================] - 138s 478us/step - loss: 0.6790 - accuracy: 0.5661\n",
      "Epoch 7/30\n",
      "288107/288107 [==============================] - 131s 456us/step - loss: 0.6779 - accuracy: 0.5681\n",
      "Epoch 8/30\n",
      "288107/288107 [==============================] - 125s 436us/step - loss: 0.6773 - accuracy: 0.5698\n",
      "Epoch 9/30\n",
      "288107/288107 [==============================] - 133s 463us/step - loss: 0.6759 - accuracy: 0.5727\n",
      "Epoch 10/30\n",
      "288107/288107 [==============================] - 137s 477us/step - loss: 0.6753 - accuracy: 0.5742\n",
      "Epoch 11/30\n",
      "288107/288107 [==============================] - 137s 477us/step - loss: 0.6748 - accuracy: 0.5745\n",
      "Epoch 12/30\n",
      "288107/288107 [==============================] - 137s 477us/step - loss: 0.6746 - accuracy: 0.5749\n",
      "Epoch 13/30\n",
      "288107/288107 [==============================] - 138s 478us/step - loss: 0.6740 - accuracy: 0.5757\n",
      "Epoch 14/30\n",
      "288107/288107 [==============================] - 138s 479us/step - loss: 0.6736 - accuracy: 0.5766\n",
      "Epoch 15/30\n",
      "288107/288107 [==============================] - 138s 478us/step - loss: 0.6731 - accuracy: 0.5772\n",
      "Epoch 16/30\n",
      "288107/288107 [==============================] - 135s 468us/step - loss: 0.6728 - accuracy: 0.5796\n",
      "Epoch 17/30\n",
      "288107/288107 [==============================] - 135s 470us/step - loss: 0.6723 - accuracy: 0.5798\n",
      "Epoch 18/30\n",
      "288107/288107 [==============================] - 135s 467us/step - loss: 0.6724 - accuracy: 0.5790\n",
      "Epoch 19/30\n",
      "288107/288107 [==============================] - 135s 469us/step - loss: 0.6723 - accuracy: 0.5784\n",
      "Epoch 20/30\n",
      "288107/288107 [==============================] - 135s 470us/step - loss: 0.6717 - accuracy: 0.5798\n",
      "Epoch 21/30\n",
      "288107/288107 [==============================] - 136s 471us/step - loss: 0.6715 - accuracy: 0.5801\n",
      "Epoch 22/30\n",
      "288107/288107 [==============================] - 138s 478us/step - loss: 0.6716 - accuracy: 0.5808\n",
      "Epoch 23/30\n",
      "288107/288107 [==============================] - 138s 479us/step - loss: 0.6714 - accuracy: 0.5812\n",
      "Epoch 24/30\n",
      "288107/288107 [==============================] - 138s 479us/step - loss: 0.6706 - accuracy: 0.5820\n",
      "Epoch 25/30\n",
      "288107/288107 [==============================] - 138s 481us/step - loss: 0.6714 - accuracy: 0.5805\n",
      "Epoch 26/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "288107/288107 [==============================] - 124s 430us/step - loss: 0.6712 - accuracy: 0.5803\n",
      "Epoch 27/30\n",
      "288107/288107 [==============================] - 124s 431us/step - loss: 0.6708 - accuracy: 0.5822\n",
      "Epoch 28/30\n",
      "288107/288107 [==============================] - 124s 430us/step - loss: 0.6705 - accuracy: 0.5826\n",
      "Epoch 29/30\n",
      "288107/288107 [==============================] - 124s 431us/step - loss: 0.6706 - accuracy: 0.5816\n",
      "Epoch 30/30\n",
      "288107/288107 [==============================] - 124s 430us/step - loss: 0.6701 - accuracy: 0.5824\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import random\n",
    "import pickle\n",
    "import collections\n",
    "import tensorflow as tf\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords \n",
    "from sklearn.externals import joblib\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.models import Sequential\n",
    "from keras.models import load_model\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Bidirectional\n",
    "from keras.layers import GRU\n",
    "from keras.layers import SimpleRNN\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing import text\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "### Preprocessing variables\n",
    "DIMENSIONS = ['IE', 'NS', 'FT', 'PJ']\n",
    "MODEL_BATCH_SIZE = 128\n",
    "TOP_WORDS = 2500\n",
    "MAX_POST_LENGTH = 40\n",
    "EMBEDDING_VECTOR_LENGTH = 50\n",
    "\n",
    "### Learning variables\n",
    "LEARNING_RATE = 0.01\n",
    "DROPOUT = 0.1\n",
    "NUM_EPOCHS = 30\n",
    "\n",
    "### Control variables\n",
    "CROSS_VALIDATION = False\n",
    "SAMPLE = False\n",
    "WORD_CLOUD = True\n",
    "\n",
    "for k in range(len(DIMENSIONS)):\n",
    "\t\n",
    "\t###########################\n",
    "\t### POST CLASSIFICATION ###\n",
    "\t###########################\n",
    "\n",
    "\t### Read in data\n",
    "\tx_train = [] \n",
    "\ty_train = [] \n",
    "\tx_test = [] \n",
    "\ty_test = []\n",
    "\twith open('train_{}.csv'.format(DIMENSIONS[k][0]), 'r') as f: \n",
    "\t\treader = csv.reader(f)\n",
    "\t\tfor row in reader:\n",
    "\t\t\tfor post in row: \n",
    "\t\t\t\tx_train.append(post)\n",
    "\t\t\t\ty_train.append(0)\n",
    "\twith open('train_{}.csv'.format(DIMENSIONS[k][1]), 'r') as f: \n",
    "\t\treader = csv.reader(f)\n",
    "\t\tfor row in reader:\n",
    "\t\t\tfor post in row: \n",
    "\t\t\t\tx_train.append(post)\n",
    "\t\t\t\ty_train.append(1)\n",
    "\twith open('test_{}.csv'.format(DIMENSIONS[k][0]), 'r') as f: \n",
    "\t\treader = csv.reader(f)\n",
    "\t\tfor row in reader:\n",
    "\t\t\tfor post in row: \n",
    "\t\t\t\tx_test.append(post)\n",
    "\t\t\t\ty_test.append(0)\n",
    "\twith open('test_{}.csv'.format(DIMENSIONS[k][1]), 'r') as f: \n",
    "\t\treader = csv.reader(f)\n",
    "\t\tfor row in reader:\n",
    "\t\t\tfor post in row: \n",
    "\t\t\t\tx_test.append(post)\n",
    "\t\t\t\ty_test.append(1)\n",
    "\n",
    "\t### Preprocessing (lemmatization, tokenization, and padding of input\n",
    "\ttypes = ['INFJ', 'ENTP', 'INTP', 'INTJ', 'ENTJ', 'ENFJ', 'INFP', 'ENFP',\n",
    "\t\t\t 'ISFP', 'ISTP', 'ISFJ', 'ISTJ', 'ESTP', 'ESFP', 'ESTJ', 'ESFJ']\n",
    "\ttypes = [x.lower() for x in types]\n",
    "\tlemmatizer = WordNetLemmatizer()\n",
    "\tstop_words = stopwords.words(\"english\")\n",
    "\tdef lemmatize(x):\n",
    "\t\tlemmatized = []\n",
    "\t\tfor post in x: \n",
    "\t\t\ttemp = post.lower() \n",
    "\t\t\tfor type_ in types: \n",
    "\t\t\t\ttemp = temp.replace(' ' + type_, '')\n",
    "\t\t\ttemp = ' '.join([lemmatizer.lemmatize(word) for word in temp.split(' ') if (word not in stop_words)])\n",
    "\t\t\tlemmatized.append(temp)\n",
    "\t\treturn np.array(lemmatized)\n",
    "\ttokenizer = text.Tokenizer(num_words=TOP_WORDS, split=' ')\n",
    "\ttokenizer.fit_on_texts(lemmatize(x_train))\n",
    "\tdef preprocess(x):\n",
    "\t\tlemmatized = lemmatize(x)\n",
    "\t\ttokenized = tokenizer.texts_to_sequences(lemmatized)\n",
    "\t\treturn sequence.pad_sequences(tokenized, maxlen=MAX_POST_LENGTH)\n",
    "\n",
    "\t### Assign to dataframe and shuffle rows\n",
    "\tdf = pd.DataFrame(data={'x': x_train, 'y': y_train})\n",
    "\tdf = df.sample(frac=1).reset_index(drop=True) ### Shuffle rows\n",
    "\tif SAMPLE:\n",
    "\t\tdf = df.head(10000) ### Small sample for quick runs\n",
    "\t\n",
    "\t### Load glove into memory for embedding\n",
    "\tembeddings_index = dict()\n",
    "\twith open('glove.6B.50d.txt') as f: \n",
    "\t\tfor line in f:\n",
    "\t\t\tvalues = line.split()\n",
    "\t\t\tword = values[0]\n",
    "\t\t\tembeddings_index[word] = np.asarray(values[1:], dtype='float32')\n",
    "\tprint('Loaded {} word vectors.'.format(len(embeddings_index)))\n",
    "\t\n",
    "\t### Create a weight matrix for words\n",
    "\tembedding_matrix = np.zeros((TOP_WORDS, EMBEDDING_VECTOR_LENGTH))\n",
    "\tfor word, i in tokenizer.word_index.items():\n",
    "\t\tif i < TOP_WORDS: \n",
    "\t\t\tembedding_vector = embeddings_index.get(word)\n",
    "\t\t\tif embedding_vector is not None:\n",
    "\t\t\t\tembedding_matrix[i] = embedding_vector\n",
    "\n",
    "\t### Construct model\n",
    "\twith tf.device('/gpu:0'):\n",
    "\t\tmodel = Sequential()\n",
    "\t\tmodel.add(Embedding(TOP_WORDS, EMBEDDING_VECTOR_LENGTH, input_length=MAX_POST_LENGTH, \n",
    "\t\t\t\t\t\t\tweights=[embedding_matrix], mask_zero=True, trainable=True))\n",
    "\t\t#model.add(SimpleRNN(EMBEDDING_VECTOR_LENGTH, dropout=DROPOUT, recurrent_dropout=DROPOUT, activation='sigmoid', kernel_initializer='zeros'))\n",
    "\t\t#model.add(GRU(EMBEDDING_VECTOR_LENGTH, dropout=DROPOUT, recurrent_dropout=DROPOUT, activation='sigmoid', kernel_initializer='zeros'))\n",
    "\t\tmodel.add(LSTM(EMBEDDING_VECTOR_LENGTH, dropout=DROPOUT, recurrent_dropout=DROPOUT, activation='sigmoid', kernel_initializer='zeros'))\n",
    "\t\t#model.add(Bidirectional(LSTM(EMBEDDING_VECTOR_LENGTH, dropout=DROPOUT, recurrent_dropout=DROPOUT, activation='sigmoid', kernel_initializer='zeros')))\n",
    "\t\tmodel.add(Dense(1, activation='sigmoid'))\n",
    "\t\toptimizer = Adam(lr=LEARNING_RATE, beta_1=0.9, beta_2=0.999, epsilon=1e-8)\n",
    "\t\tmodel.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\t\tprint(model.summary())\n",
    "\n",
    "\t\t### Cross-validation classification (individual posts)\n",
    "\t\tif CROSS_VALIDATION: \n",
    "\t\t\tk_fold = KFold(n=len(x_train), n_folds=5)\n",
    "\t\t\tscores_k = []\n",
    "\t\t\tconfusion_k = np.array([[0, 0], [0, 0]])\n",
    "\t\t\tfor train_indices, test_indices in k_fold:\n",
    "\t\t\t\tx_train_k = df.iloc[train_indices]['x'].values\n",
    "\t\t\t\ty_train_k = df.iloc[train_indices]['y'].values\n",
    "\t\t\t\tx_test_k = df.iloc[test_indices]['x'].values\n",
    "\t\t\t\ty_test_k = df.iloc[test_indices]['y'].values\n",
    "\t\t\t\tmodel.fit(preprocess(x_train_k), y_train_k, epochs=NUM_EPOCHS, batch_size=MODEL_BATCH_SIZE)\n",
    "\t\t\t\tpredictions_k = model.predict_classes(preprocess(x_test_k))\n",
    "\t\t\t\tconfusion_k += confusion_matrix(y_test_k, predictions_k)\n",
    "\t\t\t\tscore_k = accuracy_score(y_test_k, predictions_k)\n",
    "\t\t\t\tscores_k.append(score_k)\n",
    "\t\t\twith open('cross_validation_{}.txt'.format(DIMENSIONS[k]), 'w') as f: \n",
    "\t\t\t\tf.write('*** {}/{} TRAINING SET CROSS VALIDATION (POSTS) ***\\n'.format(DIMENSIONS[k][0], DIMENSIONS[k][1]))\n",
    "\t\t\t\tf.write('Total posts classified: {}\\n'.format(len(x_train)))\n",
    "\t\t\t\tf.write('Accuracy: {}\\n'.format(sum(scores_k)/len(scores_k)))\n",
    "\t\t\t\tf.write('Confusion matrix: \\n')\n",
    "\t\t\t\tf.write(np.array2string(confusion_k, separator=', '))\n",
    "\t\t\n",
    "\t\t### Test set classification (individual posts)\n",
    "\t\tmodel.fit(preprocess(df['x'].values), df['y'].values, epochs=NUM_EPOCHS, batch_size=MODEL_BATCH_SIZE)\n",
    "\t\tpredictions = model.predict_classes(preprocess(x_test)) \n",
    "\t\tconfusion = confusion_matrix(y_test, predictions)\n",
    "\t\tscore = accuracy_score(y_test, predictions)\n",
    "\t\twith open('test_set_post_classification_{}.txt'.format(DIMENSIONS[k]), 'w') as f: \n",
    "\t\t\tf.write('*** {}/{} TEST SET CLASSIFICATION (POSTS) ***\\n'.format(DIMENSIONS[k][0], DIMENSIONS[k][1]))\n",
    "\t\t\tf.write('Total posts classified: {}\\n'.format(len(x_test)))\n",
    "\t\t\tf.write('Accuracy: {}\\n'.format(score))\n",
    "\t\t\tf.write('Confusion matrix: \\n')\n",
    "\t\t\tf.write(np.array2string(confusion, separator=', '))\n",
    "\n",
    "\t\t### Get most a-like/b-like sentences \n",
    "\t\tif WORD_CLOUD:\n",
    "\t\t\tNUM_EXTREME_EXAMPLES = 500\n",
    "\t\t\tprobs = model.predict_proba(preprocess(x_test))\n",
    "\t\t\tscores = []\n",
    "\t\t\tindices = []\n",
    "\t\t\tfor i, prob in enumerate(probs, 0): \n",
    "\t\t\t\tscores.append(prob[0])\n",
    "\t\t\t\tindices.append(i)\n",
    "\t\t\tsorted_probs = sorted(zip(scores, indices))\n",
    "\t\t\tmin_prob_indices = sorted_probs[:NUM_EXTREME_EXAMPLES]\n",
    "\t\t\tmax_prob_indices = sorted_probs[-NUM_EXTREME_EXAMPLES:]\n",
    "\t\t\tlemmatized = lemmatize(x_test)\n",
    "\t\t\twith open('extreme_examples_{}.txt'.format(DIMENSIONS[k][0]), 'w') as f: \n",
    "\t\t\t\tfor prob, i in min_prob_indices:\n",
    "\t\t\t\t\t#f.write(x_test[i]+'\\n')\n",
    "\t\t\t\t\tf.write(lemmatized[i]+'\\n')\n",
    "\t\t\t\t\t#f.write(str(prob)+'\\n')\n",
    "\t\t\t\t\tf.write('\\n')\n",
    "\t\t\twith open('extreme_examples_{}.txt'.format(DIMENSIONS[k][1]), 'w') as f: \n",
    "\t\t\t\tfor prob, i in max_prob_indices:\n",
    "\t\t\t\t\t#f.write(x_test[i]+'\\n')\n",
    "\t\t\t\t\tf.write(lemmatized[i]+'\\n')\n",
    "\t\t\t\t\t#f.write(str(prob)+'\\n')\n",
    "\t\t\t\t\tf.write('\\n')\n",
    "\t\t\n",
    "\t\t### Save model and tokenizer for future use\n",
    "\t\tmodel.save('model_{}.h5'.format(DIMENSIONS[k]))\n",
    "\t\twith open('tokenizer_{}.pkl'.format(DIMENSIONS[k]), 'wb') as f:\n",
    "\t\t\tpickle.dump(tokenizer, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\t###########################\n",
    "\t### USER CLASSIFICATION ###\n",
    "\t###########################\n",
    "\t\n",
    "\t### Read in user test set data\n",
    "\tx_test_a = []\n",
    "\tx_test_b = []\n",
    "\twith open('test_{}.csv'.format(DIMENSIONS[k][0]), 'r') as f: \n",
    "\t\treader = csv.reader(f)\n",
    "\t\tfor row in reader:\n",
    "\t\t\tx_test_a.append(row)\n",
    "\twith open('test_{}.csv'.format(DIMENSIONS[k][1]), 'r') as f: \n",
    "\t\treader = csv.reader(f)\n",
    "\t\tfor row in reader: \n",
    "\t\t\tx_test_b.append(row)\n",
    "\tx_test = x_test_a + x_test_b\n",
    "\n",
    "\t### Make predictions for users\n",
    "\tpredictions_a = []\n",
    "\tfor user_batch in x_test_a:\n",
    "\t\tpredictions_for_batch = model.predict_classes(preprocess(user_batch))\n",
    "\t\tpredictions_for_batch = [item for sublist in predictions_for_batch for item in sublist] ### Make flat list\n",
    "\t\tprediction = collections.Counter(predictions_for_batch).most_common(1)[0][0]\n",
    "\t\tpredictions_a.append(prediction)\n",
    "\tpredictions_b = []\n",
    "\tfor user_batch in x_test_b:\n",
    "\t\tpredictions_for_batch = model.predict_classes(preprocess(user_batch))\n",
    "\t\tpredictions_for_batch = [item for sublist in predictions_for_batch for item in sublist] ### Make flat list\n",
    "\t\tprediction = collections.Counter(predictions_for_batch).most_common(1)[0][0]\n",
    "\t\tpredictions_b.append(prediction)\n",
    "\tpredictions = predictions_a + predictions_b\n",
    "\n",
    "\t### Analyze user classification results\n",
    "\ty_test_a = [0 for ___ in predictions_a]\n",
    "\ty_test_b = [1 for ___ in predictions_b]\n",
    "\ty_test = y_test_a + y_test_b\n",
    "\tconfusion = confusion_matrix(y_test, predictions)\n",
    "\tscore = accuracy_score(y_test, predictions)\n",
    "\twith open('test_set_user_classification_{}.txt'.format(DIMENSIONS[k]), 'w') as f: \n",
    "\t\tf.write('*** {}/{} TEST SET CLASSIFICATION (USERS) ***\\n'.format(DIMENSIONS[k][0], DIMENSIONS[k][1]))\n",
    "\t\tf.write('Total posts classified: {}\\n'.format(len(x_test)))\n",
    "\t\tf.write('Accuracy: {}\\n'.format(score))\n",
    "\t\tf.write('Confusion matrix: \\n')\n",
    "\t\tf.write(np.array2string(confusion, separator=', '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** I/E TEST SET CLASSIFICATION (USERS) ***\n",
      "Total posts classified: 463\n",
      "Accuracy: 0.6112311015118791\n",
      "Confusion matrix: \n",
      "[[168,  67],\n",
      " [113, 115]]\n",
      "*** N/S TEST SET CLASSIFICATION (USERS) ***\n",
      "Total posts classified: 463\n",
      "Accuracy: 0.5809935205183585\n",
      "Confusion matrix: \n",
      "[[ 47, 162],\n",
      " [ 32, 222]]\n",
      "*** F/T TEST SET CLASSIFICATION (USERS) ***\n",
      "Total posts classified: 463\n",
      "Accuracy: 0.6976241900647948\n",
      "Confusion matrix: \n",
      "[[134,  98],\n",
      " [ 42, 189]]\n",
      "*** P/J TEST SET CLASSIFICATION (USERS) ***\n",
      "Total posts classified: 463\n",
      "Accuracy: 0.4794816414686825\n",
      "Confusion matrix: \n",
      "[[194,   9],\n",
      " [232,  28]]\n"
     ]
    }
   ],
   "source": [
    "for k in range(len(DIMENSIONS)):\n",
    "\twith open('test_set_user_classification_{}.txt'.format(DIMENSIONS[k]), 'r') as f: \n",
    "\t\t\tprint(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
