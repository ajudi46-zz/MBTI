{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "import random\n",
    "import collections\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "df = pd.read_csv('mbti_1.csv')\n",
    "\n",
    "types = ('ISTJ', 'ISFJ', 'INFJ', 'INTJ', \\\n",
    "\t\t 'ISTP', 'ISFP', 'INFP', 'INTP', \\\n",
    "\t\t 'ESTP', 'ESFP', 'ENFP', 'ENTP', \\\n",
    "\t\t 'ESTJ', 'ESFJ', 'ENFJ', 'ENTJ')\n",
    "\n",
    "gen_pop = {'ISTJ': 0.11, 'ISFJ': 0.09, 'INFJ': 0.04, 'INTJ': 0.05, \\\n",
    "\t\t   'ISTP': 0.05, 'ISFP': 0.05, 'INFP': 0.06, 'INTP': 0.06, \\\n",
    "\t\t   'ESTP': 0.04, 'ESFP': 0.04, 'ENFP': 0.08, 'ENTP': 0.06, \\\n",
    "\t\t   'ESTJ': 0.08, 'ESFJ': 0.09, 'ENFJ': 0.05, 'ENTJ': 0.05}\n",
    "\n",
    "n, ___ = df.shape\n",
    "\n",
    "counts = collections.defaultdict(int)\n",
    "for mbti in df['type']:\n",
    "\tcounts[mbti] += 1\n",
    "\n",
    "limiting_type = None\n",
    "min_size = float('infinity')\n",
    "for mbti in counts.keys():\n",
    "\tsize = counts[mbti]/gen_pop[mbti]\n",
    "\tif size < min_size:\n",
    "\t\tmin_size = size\n",
    "\t\tlimiting_type = mbti\n",
    "\n",
    "dic = collections.defaultdict(list)\n",
    "for row in df.iterrows():\n",
    "\tdic[row[1]['type']].append(row)\n",
    "\n",
    "unclean_list = []\n",
    "\n",
    "with open('mbti_clean.csv', 'w', encoding = 'UTF-8') as f:\n",
    "\twriter = csv.writer(f)\n",
    "\twriter.writerow(['type', 'posts'])\n",
    "\t\n",
    "\tfor mbti in gen_pop.keys():\n",
    "\t\tlist1 = dic[mbti]\n",
    "\t\tfor x in range(0, int(round(min_size*gen_pop[mbti]))):\n",
    "\t\t\twriter.writerow(list1[x][1])\t\n",
    "\t\tunclean_list.append(list1[int(round(min_size*gen_pop[mbti])) : len(list1)])\n",
    "\t\t\t\t\t\n",
    "with open('mbti_unclean.csv', 'w', encoding = 'UTF-8') as f:\n",
    "\twriter = csv.writer(f)\n",
    "\twriter.writerow(['type', 'posts'])\n",
    "\tfor mbti in unclean_list:\n",
    "\t\tfor x in mbti:\n",
    "\t\t\twriter.writerow(x[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import random\n",
    "import collections\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import re\n",
    "\n",
    "df = pd.read_csv('mbti_unclean.csv')\n",
    "DIMENSIONS = ('IE', 'NS', 'TF', 'PJ')\n",
    "counts = collections.defaultdict(int)\n",
    "\n",
    "for dimension in DIMENSIONS: \n",
    "\tletter_1, letter_2 = dimension\n",
    "\tfor mbti in df['type']:\n",
    "\t\tif letter_1 in mbti:\n",
    "\t\t\tcounts[letter_1] += 1\n",
    "\t\tif letter_2 in mbti:\n",
    "\t\t\tcounts[letter_2] += 1\n",
    "\n",
    "for dimension in DIMENSIONS: \n",
    "\tletter_1, letter_2 = dimension\n",
    "\tif counts[letter_1] < counts[letter_2]:\n",
    "\t\tlimit = counts[letter_1]\n",
    "\telse: \n",
    "\t\tlimit = counts[letter_2]\n",
    "\tlist1 = []\n",
    "\tlist2 = []\n",
    "\ti = 0\n",
    "\tfor row in df.iterrows():\t\n",
    "\t\tif i == limit: break\n",
    "\t\tif letter_1 in row[1]['type']:\n",
    "\t\t\tlist1.append(row[1]['posts'].split('|||'))\n",
    "\t\t\ti += 1\n",
    "\ti = 0\n",
    "\tfor row in df.iterrows():\t\n",
    "\t\tif i == limit: break\n",
    "\t\tif letter_2 in row[1]['type']:\n",
    "\t\t\tlist2.append(row[1]['posts'].split('|||'))\t\n",
    "\t\t\ti += 1\n",
    "\twith open('train_' + letter_1 + '.csv', 'w',encoding = 'UTF-8') as f:\n",
    "\t\twriter = csv.writer(f)\n",
    "\t\tfor hundred_posts in list1:\n",
    "\t\t\trow = [post for post in hundred_posts if ('http' not in post) and (post != '') and (post != None) and (re.search(\"[a-zA-Z]\", post))]\n",
    "\t\t\twriter.writerow(row)\n",
    "\twith open('train_' + letter_2 + '.csv', 'w',encoding = 'UTF-8') as f:\n",
    "\t\twriter = csv.writer(f)\n",
    "\t\tfor hundred_posts in list2:\n",
    "\t\t\trow = [post for post in hundred_posts if ('http' not in post) and (post != '') and (post != None) and (re.search(\"[a-zA-Z]\", post))]\n",
    "\t\t\twriter.writerow(row)\n",
    "\n",
    "\n",
    "\n",
    "\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "import random\n",
    "import collections\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import re\n",
    "\n",
    "letters = ('I', 'E', 'N', 'S', 'T', 'F', 'P', 'J')\n",
    "df = pd.read_csv('mbti_clean.csv')\n",
    "test = collections.defaultdict(list) \n",
    "\n",
    "for row in df.iterrows():\n",
    "\tposts = row[1]['posts'].split('|||')\n",
    "\ttest[row[1]['type']].append(posts)\n",
    "\n",
    "### write csv files for every every letter class and train vs. test class (16 total)\n",
    "for letter in letters: \n",
    "\tPATH = 'test_' + letter + '.csv'\n",
    "\twith open(PATH, 'w',encoding = 'UTF-8') as f:\n",
    "\t\twriter = csv.writer(f)\n",
    "\t\tfor key in test.keys():\n",
    "\t\t\tif letter in key:\n",
    "\t\t\t\tfor hundred_posts in test[key]:\n",
    "\t\t\t\t\trow = [post for post in hundred_posts if ('http' not in post) and (post != '') and (post != None) and (re.search(\"[a-zA-Z]\", post))]\n",
    "\t\t\t\t\tif len(row) > 10: \n",
    "\t\t\t\t\t\twriter.writerow(row)\n",
    "\n",
    "\n",
    "\n",
    "\t\t\t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ajudi46/anaconda3/envs/pronew/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/ajudi46/anaconda3/envs/pronew/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/ajudi46/anaconda3/envs/pronew/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/ajudi46/anaconda3/envs/pronew/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/ajudi46/anaconda3/envs/pronew/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/ajudi46/anaconda3/envs/pronew/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/ajudi46/anaconda3/envs/pronew/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/ajudi46/anaconda3/envs/pronew/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/ajudi46/anaconda3/envs/pronew/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/ajudi46/anaconda3/envs/pronew/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/ajudi46/anaconda3/envs/pronew/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/ajudi46/anaconda3/envs/pronew/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/ajudi46/anaconda3/envs/pronew/lib/python3.7/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import random\n",
    "import pickle\n",
    "import collections\n",
    "import tensorflow as tf\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords \n",
    "from sklearn.externals import joblib\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.models import Sequential\n",
    "from keras.models import load_model\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Bidirectional\n",
    "from keras.layers import GRU\n",
    "from keras.layers import SimpleRNN\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing import text\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "### Preprocessing variables\n",
    "DIMENSIONS = ['IE', 'NS', 'FT', 'PJ']\n",
    "MODEL_BATCH_SIZE = 128\n",
    "TOP_WORDS = 2500\n",
    "MAX_POST_LENGTH = 40\n",
    "EMBEDDING_VECTOR_LENGTH = 50\n",
    "\n",
    "### Learning variables\n",
    "LEARNING_RATE = 0.01\n",
    "DROPOUT = 0.1\n",
    "NUM_EPOCHS = 30\n",
    "\n",
    "### Control variables\n",
    "CROSS_VALIDATION = False\n",
    "SAMPLE = False\n",
    "WORD_CLOUD = True\n",
    "\n",
    "for k in range(len(DIMENSIONS)):\n",
    "\t\n",
    "\t###########################\n",
    "\t### POST CLASSIFICATION ###\n",
    "\t###########################\n",
    "\n",
    "\t### Read in data\n",
    "\tx_train = [] \n",
    "\ty_train = [] \n",
    "\tx_test = [] \n",
    "\ty_test = []\n",
    "\twith open('train_{}.csv'.format(DIMENSIONS[k][0]), 'r',encoding = 'UTF-8') as f: \n",
    "\t\treader = csv.reader(f)\n",
    "\t\tfor row in reader:\n",
    "\t\t\tfor post in row: \n",
    "\t\t\t\tx_train.append(post)\n",
    "\t\t\t\ty_train.append(0)\n",
    "\twith open('train_{}.csv'.format(DIMENSIONS[k][1]), 'r',encoding = 'UTF-8') as f: \n",
    "\t\treader = csv.reader(f)\n",
    "\t\tfor row in reader:\n",
    "\t\t\tfor post in row: \n",
    "\t\t\t\tx_train.append(post)\n",
    "\t\t\t\ty_train.append(1)\n",
    "\twith open('test_{}.csv'.format(DIMENSIONS[k][0]), 'r',encoding = 'UTF-8') as f: \n",
    "\t\treader = csv.reader(f)\n",
    "\t\tfor row in reader:\n",
    "\t\t\tfor post in row: \n",
    "\t\t\t\tx_test.append(post)\n",
    "\t\t\t\ty_test.append(0)\n",
    "\twith open('test_{}.csv'.format(DIMENSIONS[k][1]), 'r',encoding = 'UTF-8') as f: \n",
    "\t\treader = csv.reader(f)\n",
    "\t\tfor row in reader:\n",
    "\t\t\tfor post in row: \n",
    "\t\t\t\tx_test.append(post)\n",
    "\t\t\t\ty_test.append(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400000 word vectors.\n"
     ]
    },
    {
     "ename": "InternalError",
     "evalue": "CUDA runtime implicit initialization on GPU:0 failed. Status: all CUDA-capable devices are busy or unavailable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-7ed7f6568176>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \tmodel.add(Embedding(TOP_WORDS, EMBEDDING_VECTOR_LENGTH, input_length=MAX_POST_LENGTH, \n\u001b[0;32m---> 50\u001b[0;31m \t\t\t\t\t\tweights=[embedding_matrix], mask_zero=True, trainable=True))\n\u001b[0m\u001b[1;32m     51\u001b[0m         \u001b[0;31m#model.add(SimpleRNN(EMBEDDING_VECTOR_LENGTH, dropout=DROPOUT, recurrent_dropout=DROPOUT, activation='sigmoid', kernel_initializer='zeros'))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;31m#model.add(GRU(EMBEDDING_VECTOR_LENGTH, dropout=DROPOUT, recurrent_dropout=DROPOUT, activation='sigmoid', kernel_initializer='zeros'))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pronew/lib/python3.7/site-packages/keras/engine/sequential.py\u001b[0m in \u001b[0;36madd\u001b[0;34m(self, layer)\u001b[0m\n\u001b[1;32m    164\u001b[0m                     \u001b[0;31m# and create the node connecting the current layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m                     \u001b[0;31m# to the input layer we just created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m                     \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m                     \u001b[0mset_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pronew/lib/python3.7/site-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    466\u001b[0m                 \u001b[0;31m# Load weights that were specified at layer instantiation.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initial_weights\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 468\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initial_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    469\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m             \u001b[0;31m# Raise exceptions in case the input is not compatible\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pronew/lib/python3.7/site-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36mset_weights\u001b[0;34m(self, weights)\u001b[0m\n\u001b[1;32m   1118\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m         \u001b[0mweight_value_tuples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1120\u001b[0;31m         \u001b[0mparam_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_get_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1121\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mpv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pronew/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mbatch_get_value\u001b[0;34m(ops)\u001b[0m\n\u001b[1;32m   2937\u001b[0m         \u001b[0mA\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mNumpy\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2938\u001b[0m     \"\"\"\n\u001b[0;32m-> 2939\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtf_keras_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_get_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2940\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2941\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pronew/lib/python3.7/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36mbatch_get_value\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m   3008\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Cannot get value inside Tensorflow graph function.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3009\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3010\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3011\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3012\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pronew/lib/python3.7/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36mget_session\u001b[0;34m(op_input_list)\u001b[0m\n\u001b[1;32m    457\u001b[0m       \u001b[0mA\u001b[0m \u001b[0mTensorFlow\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m   \"\"\"\n\u001b[0;32m--> 459\u001b[0;31m   \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_input_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    460\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_MANUAL_VAR_INIT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pronew/lib/python3.7/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m_get_session\u001b[0;34m(op_input_list)\u001b[0m\n\u001b[1;32m    429\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m         _SESSION.session = session_module.Session(\n\u001b[0;32m--> 431\u001b[0;31m             config=get_default_session_config())\n\u001b[0m\u001b[1;32m    432\u001b[0m     \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_SESSION\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pronew/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, target, graph, config)\u001b[0m\n\u001b[1;32m   1568\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1569\u001b[0m     \"\"\"\n\u001b[0;32m-> 1570\u001b[0;31m     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1571\u001b[0m     \u001b[0;31m# NOTE(mrry): Create these on first `__enter__` to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1572\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_default_graph_context_manager\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pronew/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, target, graph, config)\u001b[0m\n\u001b[1;32m    691\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    692\u001b[0m       \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 693\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_NewSessionRef\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_c_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    694\u001b[0m       \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    695\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInternalError\u001b[0m: CUDA runtime implicit initialization on GPU:0 failed. Status: all CUDA-capable devices are busy or unavailable"
     ]
    }
   ],
   "source": [
    "\t### Preprocessing (lemmatization, tokenization, and padding of input\n",
    "\ttypes = ['INFJ', 'ENTP', 'INTP', 'INTJ', 'ENTJ', 'ENFJ', 'INFP', 'ENFP',\n",
    "\t\t\t 'ISFP', 'ISTP', 'ISFJ', 'ISTJ', 'ESTP', 'ESFP', 'ESTJ', 'ESFJ']\n",
    "\ttypes = [x.lower() for x in types]\n",
    "\tlemmatizer = WordNetLemmatizer()\n",
    "\tstop_words = stopwords.words(\"english\")\n",
    "\tdef lemmatize(x):\n",
    "\t\tlemmatized = []\n",
    "\t\tfor post in x: \n",
    "\t\t\ttemp = post.lower() \n",
    "\t\t\tfor type_ in types: \n",
    "\t\t\t\ttemp = temp.replace(' ' + type_, '')\n",
    "\t\t\ttemp = ' '.join([lemmatizer.lemmatize(word) for word in temp.split(' ') if (word not in stop_words)])\n",
    "\t\t\tlemmatized.append(temp)\n",
    "\t\treturn np.array(lemmatized)\n",
    "\ttokenizer = text.Tokenizer(num_words=TOP_WORDS, split=' ')\n",
    "\ttokenizer.fit_on_texts(lemmatize(x_train))\n",
    "\tdef preprocess(x):\n",
    "\t\tlemmatized = lemmatize(x)\n",
    "\t\ttokenized = tokenizer.texts_to_sequences(lemmatized)\n",
    "\t\treturn sequence.pad_sequences(tokenized, maxlen=MAX_POST_LENGTH)\n",
    "\n",
    "\t### Assign to dataframe and shuffle rows\n",
    "\tdf = pd.DataFrame(data={'x': x_train, 'y': y_train})\n",
    "\tdf = df.sample(frac=1).reset_index(drop=True) ### Shuffle rows\n",
    "\tif SAMPLE:\n",
    "\t\tdf = df.head(10000) ### Small sample for quick runs\n",
    "\t\n",
    "\t### Load glove into memory for embedding\n",
    "\tembeddings_index = dict()\n",
    "\twith open('glove.6B.50d.txt',encoding = 'UTF-8') as f: \n",
    "\t\tfor line in f:\n",
    "\t\t\tvalues = line.split()\n",
    "\t\t\tword = values[0]\n",
    "\t\t\tembeddings_index[word] = np.asarray(values[1:], dtype='float32')\n",
    "\tprint('Loaded {} word vectors.'.format(len(embeddings_index)))\n",
    "\t\n",
    "\t### Create a weight matrix for words\n",
    "\tembedding_matrix = np.zeros((TOP_WORDS, EMBEDDING_VECTOR_LENGTH))\n",
    "\tfor word, i in tokenizer.word_index.items():\n",
    "\t\tif i < TOP_WORDS: \n",
    "\t\t\tembedding_vector = embeddings_index.get(word)\n",
    "\t\t\tif embedding_vector is not None:\n",
    "\t\t\t\tembedding_matrix[i] = embedding_vector\n",
    "\n",
    "\t### Construct model\n",
    "\twith tf.device('/gpu:0'):\n",
    "\t\tmodel = Sequential()\n",
    "\t\tmodel.add(Embedding(TOP_WORDS, EMBEDDING_VECTOR_LENGTH, input_length=MAX_POST_LENGTH, \n",
    "\t\t\t\t\t\t\tweights=[embedding_matrix], mask_zero=True, trainable=True))\n",
    "\t\t#model.add(SimpleRNN(EMBEDDING_VECTOR_LENGTH, dropout=DROPOUT, recurrent_dropout=DROPOUT, activation='sigmoid', kernel_initializer='zeros'))\n",
    "\t\t#model.add(GRU(EMBEDDING_VECTOR_LENGTH, dropout=DROPOUT, recurrent_dropout=DROPOUT, activation='sigmoid', kernel_initializer='zeros'))\n",
    "\t\tmodel.add(LSTM(EMBEDDING_VECTOR_LENGTH, dropout=DROPOUT, recurrent_dropout=DROPOUT, activation='sigmoid', kernel_initializer='zeros'))\n",
    "\t\t#model.add(Bidirectional(LSTM(EMBEDDING_VECTOR_LENGTH, dropout=DROPOUT, recurrent_dropout=DROPOUT, activation='sigmoid', kernel_initializer='zeros')))\n",
    "\t\tmodel.add(Dense(1, activation='sigmoid'))\n",
    "\t\toptimizer = Adam(lr=LEARNING_RATE, beta_1=0.9, beta_2=0.999, epsilon=1e-8)\n",
    "\t\tmodel.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\t\tprint(model.summary())\n",
    "\n",
    "\t\t### Cross-validation classification (individual posts)\n",
    "\t\tif CROSS_VALIDATION: \n",
    "\t\t\tk_fold = KFold(n=len(x_train), n_folds=5)\n",
    "\t\t\tscores_k = []\n",
    "\t\t\tconfusion_k = np.array([[0, 0], [0, 0]])\n",
    "\t\t\tfor train_indices, test_indices in k_fold:\n",
    "\t\t\t\tx_train_k = df.iloc[train_indices]['x'].values\n",
    "\t\t\t\ty_train_k = df.iloc[train_indices]['y'].values\n",
    "\t\t\t\tx_test_k = df.iloc[test_indices]['x'].values\n",
    "\t\t\t\ty_test_k = df.iloc[test_indices]['y'].values\n",
    "\t\t\t\tmodel.fit(preprocess(x_train_k), y_train_k, epochs=NUM_EPOCHS, batch_size=MODEL_BATCH_SIZE)\n",
    "\t\t\t\tpredictions_k = model.predict_classes(preprocess(x_test_k))\n",
    "\t\t\t\tconfusion_k += confusion_matrix(y_test_k, predictions_k)\n",
    "\t\t\t\tscore_k = accuracy_score(y_test_k, predictions_k)\n",
    "\t\t\t\tscores_k.append(score_k)\n",
    "\t\t\twith open('cross_validation_{}.txt'.format(DIMENSIONS[k]), 'w',encoding = 'UTF-8') as f: \n",
    "\t\t\t\tf.write('*** {}/{} TRAINING SET CROSS VALIDATION (POSTS) ***\\n'.format(DIMENSIONS[k][0], DIMENSIONS[k][1]))\n",
    "\t\t\t\tf.write('Total posts classified: {}\\n'.format(len(x_train)))\n",
    "\t\t\t\tf.write('Accuracy: {}\\n'.format(sum(scores_k)/len(scores_k)))\n",
    "\t\t\t\tf.write('Confusion matrix: \\n')\n",
    "\t\t\t\tf.write(np.array2string(confusion_k, separator=', '))\n",
    "\t\t\n",
    "\t\t### Test set classification (individual posts)\n",
    "\t\tmodel.fit(preprocess(df['x'].values), df['y'].values, epochs=NUM_EPOCHS, batch_size=MODEL_BATCH_SIZE)\n",
    "\t\tpredictions = model.predict_classes(preprocess(x_test)) \n",
    "\t\tconfusion = confusion_matrix(y_test, predictions)\n",
    "\t\tscore = accuracy_score(y_test, predictions)\n",
    "\t\twith open('test_set_post_classification_{}.txt'.format(DIMENSIONS[k]), 'w',encoding = 'UTF-8') as f: \n",
    "\t\t\tf.write('*** {}/{} TEST SET CLASSIFICATION (POSTS) ***\\n'.format(DIMENSIONS[k][0], DIMENSIONS[k][1]))\n",
    "\t\t\tf.write('Total posts classified: {}\\n'.format(len(x_test)))\n",
    "\t\t\tf.write('Accuracy: {}\\n'.format(score))\n",
    "\t\t\tf.write('Confusion matrix: \\n')\n",
    "\t\t\tf.write(np.array2string(confusion, separator=', '))\n",
    "\n",
    "\t\t### Get most a-like/b-like sentences \n",
    "\t\tif WORD_CLOUD:\n",
    "\t\t\tNUM_EXTREME_EXAMPLES = 500\n",
    "\t\t\tprobs = model.predict_proba(preprocess(x_test))\n",
    "\t\t\tscores = []\n",
    "\t\t\tindices = []\n",
    "\t\t\tfor i, prob in enumerate(probs, 0): \n",
    "\t\t\t\tscores.append(prob[0])\n",
    "\t\t\t\tindices.append(i)\n",
    "\t\t\tsorted_probs = sorted(zip(scores, indices))\n",
    "\t\t\tmin_prob_indices = sorted_probs[:NUM_EXTREME_EXAMPLES]\n",
    "\t\t\tmax_prob_indices = sorted_probs[-NUM_EXTREME_EXAMPLES:]\n",
    "\t\t\tlemmatized = lemmatize(x_test)\n",
    "\t\t\twith open('extreme_examples_{}.txt'.format(DIMENSIONS[k][0]), 'w',encoding = 'UTF-8') as f: \n",
    "\t\t\t\tfor prob, i in min_prob_indices:\n",
    "\t\t\t\t\t#f.write(x_test[i]+'\\n')\n",
    "\t\t\t\t\tf.write(lemmatized[i]+'\\n')\n",
    "\t\t\t\t\t#f.write(str(prob)+'\\n')\n",
    "\t\t\t\t\tf.write('\\n')\n",
    "\t\t\twith open('extreme_examples_{}.txt'.format(DIMENSIONS[k][1]), 'w',encoding = 'UTF-8') as f: \n",
    "\t\t\t\tfor prob, i in max_prob_indices:\n",
    "\t\t\t\t\t#f.write(x_test[i]+'\\n')\n",
    "\t\t\t\t\tf.write(lemmatized[i]+'\\n')\n",
    "\t\t\t\t\t#f.write(str(prob)+'\\n')\n",
    "\t\t\t\t\tf.write('\\n')\n",
    "\t\t\n",
    "\t\t### Save model and tokenizer for future use\n",
    "\t\tmodel.save('model_{}.h5'.format(DIMENSIONS[k]))\n",
    "\t\twith open('tokenizer_{}.pkl'.format(DIMENSIONS[k]), 'wb') as f:\n",
    "\t\t\tpickle.dump(tokenizer, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "            \n",
    "            \n",
    "    ###########################\n",
    "\t### USER CLASSIFICATION ###\n",
    "\t###########################\n",
    "\t\n",
    "\t### Read in user test set data\n",
    "\tx_test_a = []\n",
    "\tx_test_b = []\n",
    "\twith open('test_{}.csv'.format(DIMENSIONS[k][0]), 'r',encoding = 'UTF-8') as f: \n",
    "\t\treader = csv.reader(f)\n",
    "\t\tfor row in reader:\n",
    "\t\t\tx_test_a.append(row)\n",
    "\twith open('test_{}.csv'.format(DIMENSIONS[k][1]), 'r',encoding = 'UTF-8') as f: \n",
    "\t\treader = csv.reader(f)\n",
    "\t\tfor row in reader: \n",
    "\t\t\tx_test_b.append(row)\n",
    "\tx_test = x_test_a + x_test_b\n",
    "\n",
    "\t### Make predictions for users\n",
    "\tpredictions_a = []\n",
    "\tfor user_batch in x_test_a:\n",
    "\t\tpredictions_for_batch = model.predict_classes(preprocess(user_batch))\n",
    "\t\tpredictions_for_batch = [item for sublist in predictions_for_batch for item in sublist] ### Make flat list\n",
    "\t\tprediction = collections.Counter(predictions_for_batch).most_common(1)[0][0]\n",
    "\t\tpredictions_a.append(prediction)\n",
    "\tpredictions_b = []\n",
    "\tfor user_batch in x_test_b:\n",
    "\t\tpredictions_for_batch = model.predict_classes(preprocess(user_batch))\n",
    "\t\tpredictions_for_batch = [item for sublist in predictions_for_batch for item in sublist] ### Make flat list\n",
    "\t\tprediction = collections.Counter(predictions_for_batch).most_common(1)[0][0]\n",
    "\t\tpredictions_b.append(prediction)\n",
    "\tpredictions = predictions_a + predictions_b\n",
    "\n",
    "\t### Analyze user classification results\n",
    "\ty_test_a = [0 for ___ in predictions_a]\n",
    "\ty_test_b = [1 for ___ in predictions_b]\n",
    "\ty_test = y_test_a + y_test_b\n",
    "\tconfusion = confusion_matrix(y_test, predictions)\n",
    "\tscore = accuracy_score(y_test, predictions)\n",
    "\twith open('test_set_user_classification_{}.txt'.format(DIMENSIONS[k]), 'w',encoding = 'UTF-8') as f: \n",
    "\t\tf.write('*** {}/{} TEST SET CLASSIFICATION (USERS) ***\\n'.format(DIMENSIONS[k][0], DIMENSIONS[k][1]))\n",
    "\t\tf.write('Total posts classified: {}\\n'.format(len(x_test)))\n",
    "\t\tf.write('Accuracy: {}\\n'.format(score))\n",
    "\t\tf.write('Confusion matrix: \\n')\n",
    "\t\tf.write(np.array2string(confusion, separator=', '))\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
